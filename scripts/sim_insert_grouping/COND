from itertools import product

CONFIGS = [
  # Represents 64 B records, 3200 bytes per page +/- 640 bytes
  {
    "records_per_page_goal": 50,
    "records_per_page_delta": 10,
    "max_page_records": 60,
  },
  # Represents 1024 B records, 2048 bytes per page +/- 1024 bytes
  {
    "records_per_page_goal": 2,
    "records_per_page_delta": 1,
    "max_page_records": 3,
  },
]

CUSTOM_DATASET_PATH = "/spinning/kipf/bourbon_datasets/"

CUSTOM_DATASETS = [
  {"name": "amzn", "file": "amazon_reviews.txt"},
  {"name": "osm", "file": "osm_ny.txt"},
]

OVERFLOW_FRACS = [
  {"name": "half", "value": 0.5},
  {"name": "full", "value": 1},
]

ITERATIONS = 3000000
SLOPE_EPSILON = 1e3

run_experiment_group(
  name="simple",
  run="./run_simple.sh",
  experiments=[
    ExperimentInstance(
      name="uniform-{}-{}-{}".format(
        config["records_per_page_goal"],
        config["records_per_page_delta"],
        frac["name"],
      ),
      args=["uniform_20m.yml"],
      parallelizable=True,
      options={
        **config,
        "max_overflow_frac": frac["value"],
      }
    )
    for config, frac in product(CONFIGS, OVERFLOW_FRACS)
  ] + [
    ExperimentInstance(
      name="{}-{}-{}-{}".format(
        dataset["name"],
        config["records_per_page_goal"],
        config["records_per_page_delta"],
        frac["name"],
      ),
      args=[CUSTOM_DATASET_PATH + dataset["file"]],
      parallelizable=True,
      options={
        **config,
        "max_overflow_frac": frac["value"],
      }
    )
    for dataset, config, frac in product(CUSTOM_DATASETS, CONFIGS, OVERFLOW_FRACS)
  ],
)

group(
  name="relevant",
  deps=[
    ":uniform-50-10",
    ":uniform-2-1",
    ":amzn-50-10",
    ":amzn-2-1",
    ":osm-50-10",
    ":osm-2-1",
  ],
)

run_experiment_group(
  name="sequential_merge",
  run="./run_sequential.sh",
  experiments=[
    ExperimentInstance(
      name="sequential_merge-uniform-{}-{}".format(
        config["records_per_page_goal"],
        config["records_per_page_delta"],
      ),
      args=["uniform_20m.yml"],
      parallelizable=True,
      options={
        "records_per_page_goal": config["records_per_page_goal"],
        "records_per_page_delta": config["records_per_page_delta"],
        "iterations": ITERATIONS,
        "slope_epsilon": SLOPE_EPSILON,
      }
    )
    for config in CONFIGS
  ] + [
    ExperimentInstance(
      name="sequential_merge-{}-{}-{}".format(
        dataset["name"],
        config["records_per_page_goal"],
        config["records_per_page_delta"],
      ),
      args=[CUSTOM_DATASET_PATH + dataset["file"]],
      parallelizable=True,
      options={
        "records_per_page_goal": config["records_per_page_goal"],
        "records_per_page_delta": config["records_per_page_delta"],
        "iterations": ITERATIONS,
        "slope_epsilon": SLOPE_EPSILON,
      }
    )
    for dataset, config in product(CUSTOM_DATASETS, CONFIGS)
  ],
)

run_experiment_group(
  name="sequential_bulk_load",
  run="./run_sequential.sh",
  experiments=[
    ExperimentInstance(
      name="sequential_bulk_load-uniform-{}-{}".format(
        config["records_per_page_goal"],
        config["records_per_page_delta"],
      ),
      args=["uniform_20m.yml"],
      parallelizable=True,
      options={
        "records_per_page_goal": config["records_per_page_goal"],
        "records_per_page_delta": config["records_per_page_delta"],
      }
    )
    for config in CONFIGS
  ] + [
    ExperimentInstance(
      name="sequential_bulk_load-{}-{}-{}".format(
        dataset["name"],
        config["records_per_page_goal"],
        config["records_per_page_delta"],
      ),
      args=[CUSTOM_DATASET_PATH + dataset["file"]],
      parallelizable=True,
      options={
        "records_per_page_goal": config["records_per_page_goal"],
        "records_per_page_delta": config["records_per_page_delta"],
      }
    )
    for dataset, config in product(CUSTOM_DATASETS, CONFIGS)
  ]
)

group(
  name="sequential",
  deps=[
    ":sequential_bulk_load",
    ":sequential_merge",
  ],
)
