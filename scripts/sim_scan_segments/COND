from itertools import product

CONFIGS = [
  # Represents 64 B records, 3200 bytes per page +/- 640 bytes
  {
    "records_per_page_goal": 50,
    "records_per_page_delta": 10,
  },
  # Represents 1024 B records, 2048 bytes per page +/- 1024 bytes
  {
    "records_per_page_goal": 2,
    "records_per_page_delta": 1,
  },
]

CUSTOM_DATASET_PATH = "/spinning/kipf/bourbon_datasets/"

CUSTOM_DATASETS = [
  {"name": "amzn", "file": "amazon_reviews.txt"},
  {"name": "osm", "file": "osm_ny.txt"},
]

WORKLOADS = [
  "long.yml",
  "short.yml",
]

def sanitize_workload_name(workload):
  return workload.replace(".yml", "")

run_experiment_group(
  name="sim_scan_segments",
  run="./run.sh",
  experiments=[
    ExperimentInstance(
      name="uniform-{}-{}-{}".format(
        sanitize_workload_name(workload),
        config["records_per_page_goal"],
        config["records_per_page_delta"]
      ),
      args=[workload],
      options=config,
      parallelizable=True,
    )
    for config, workload in product(CONFIGS, WORKLOADS)
  ] + [
    ExperimentInstance(
      name="{}-{}-{}-{}".format(
        dataset["name"],
        sanitize_workload_name(workload),
        config["records_per_page_goal"],
        config["records_per_page_delta"],
      ),
      args=[workload],
      options={
        **config,
        "custom_dataset": CUSTOM_DATASET_PATH + dataset["file"],
      },
      parallelizable=True,
    )
    for dataset, config, workload in product(CUSTOM_DATASETS, CONFIGS, WORKLOADS)
  ],
)

run_command(
  name="combine",
  run="python3 combine.py",
  deps=[
    ":sim_scan_segments",
  ],
)
